{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCPtu2KWLTak"
      },
      "source": [
        "### Step 0: Environment Setup (Install Libraries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zSk4_eDWMNOY",
        "outputId": "7fc616b6-fcde-4715-b62e-4034f246363d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.1/284.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.5/70.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cdsapi\n",
            "  Downloading cdsapi-0.7.7-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: earthaccess in /usr/local/lib/python3.12/dist-packages (0.15.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.12.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: h5netcdf in /usr/local/lib/python3.12/dist-packages (1.7.3)\n",
            "Collecting ecmwf-datastores-client>=0.4.0 (from cdsapi)\n",
            "  Downloading ecmwf_datastores_client-0.4.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from cdsapi) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from cdsapi) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2025.2 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (2025.3.0)\n",
            "Requirement already satisfied: importlib-resources>=6.3.2 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (6.5.2)\n",
            "Requirement already satisfied: multimethod>=1.8 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (2.0.2)\n",
            "Requirement already satisfied: pqdm>=0.1 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (0.2.0)\n",
            "Requirement already satisfied: python-cmr>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (0.13.0)\n",
            "Requirement already satisfied: s3fs>=2025.2 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (2025.3.0)\n",
            "Requirement already satisfied: tenacity>=9.0 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (9.1.2)\n",
            "Requirement already satisfied: tinynetrc>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from earthaccess) (4.15.0)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netCDF4) (2025.11.12)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from h5netcdf) (3.15.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from ecmwf-datastores-client>=0.4.0->cdsapi) (25.4.0)\n",
            "Collecting multiurl>=0.3.7 (from ecmwf-datastores-client>=0.4.0->cdsapi)\n",
            "  Downloading multiurl-0.3.7-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: bounded-pool-executor in /usr/local/lib/python3.12/dist-packages (from pqdm>=0.1->earthaccess) (0.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->cdsapi) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->cdsapi) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.5.0->cdsapi) (2.5.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2025.2->earthaccess) (2.26.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs>=2025.2->earthaccess) (3.13.2)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2025.2->earthaccess) (0.13.0)\n",
            "Requirement already satisfied: botocore<1.41.6,>=1.41.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2025.2->earthaccess) (1.41.5)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2025.2->earthaccess) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2025.2->earthaccess) (6.7.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs>=2025.2->earthaccess) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2025.2->earthaccess) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2025.2->earthaccess) (1.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2025.2->earthaccess) (1.8.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2025.2->earthaccess) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs>=2025.2->earthaccess) (1.22.0)\n",
            "Downloading cdsapi-0.7.7-py2.py3-none-any.whl (12 kB)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecmwf_datastores_client-0.4.1-py3-none-any.whl (29 kB)\n",
            "Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiurl-0.3.7-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: cftime, netCDF4, multiurl, ecmwf-datastores-client, cdsapi\n",
            "Successfully installed cdsapi-0.7.7 cftime-1.6.5 ecmwf-datastores-client-0.4.1 multiurl-0.3.7 netCDF4-1.7.3\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: netcdf4 in /usr/local/lib/python3.12/dist-packages (1.7.3)\n",
            "Requirement already satisfied: cftime in /usr/local/lib/python3.12/dist-packages (from netcdf4) (1.6.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from netcdf4) (2025.11.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from netcdf4) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell first to install all dependencies for the entire project.\n",
        "! pip install -q numpy pandas matplotlib seaborn scipy scikit-learn\n",
        "! pip install -q dask distributed xarray bottleneck\n",
        "! pip install -q zarr gcsfs fsspec earthaccess\n",
        "! pip install cdsapi earthaccess xarray pandas numpy netCDF4 h5netcdf\n",
        "! pip install -q netCDF4 h5netcdf rioxarray rasterio\n",
        "! pip install netcdf4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI1TRuSrsrWz"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import cdsapi\n",
        "import earthaccess\n",
        "import shutil\n",
        "import re\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from scipy.stats import gamma, lognorm, weibull_min\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.spatial.distance import pdist\n",
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N41ouHzg40fD",
        "outputId": "a1cbb9b2-d3d8-40f7-97ca-52580c46fa0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEjDZypyXi6w"
      },
      "source": [
        "### Step 1: Ground Truth & Aux Data (CWA Stations + DEM)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#KIWI'S CHANGES\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ===== Team-fixed BASE (everyone must have the shortcut here) =====\n",
        "BASE = \"/content/drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result\"\n",
        "\n",
        "# ---- CSV patterns ----\n",
        "RAIN_PATTERN = os.path.join(BASE,\"Monthly_Average_Rainfall_Data\",\"觀測_月資料_臺灣_降雨量_*.csv\")\n",
        "TEMP_PATTERN = os.path.join(BASE,\"Monthly_Average_Temperature_Data\",\"觀測_月資料_臺灣_平均溫_*.csv\")\n",
        "\n",
        "# ---- Output directory ----\n",
        "OUT_DIR = os.path.join(BASE, \"outputs\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "OUT_RAIN_NC = os.path.join(OUT_DIR, \"monthly_rain_climatology.nc\")\n",
        "OUT_TEMP_NC = os.path.join(OUT_DIR, \"monthly_temp_climatology.nc\")\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "SawKYnkwtXgh",
        "outputId": "3b83b3b4-81ac-4c02-9f31-358529337cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-2396795403.py, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2396795403.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "collapsed": true,
        "id": "legpukUJhke7",
        "outputId": "cbc39579-4b13-495e-db13-fc25cf204de0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '/content/drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result/CWA_Rainfall_Compiled.nc'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/content/drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result/CWA_Rainfall_Compiled.nc',), 'a', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '9fb2701c-03b1-4316-9cbc-1f3857dcfa59']",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-782361629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0mds_rain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_csv_to_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRAIN_PATTERN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_RAIN_NC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Precip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0mds_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_csv_to_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMP_PATTERN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUT_TEMP_NC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Temperature\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mds_rain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-782361629.py\u001b[0m in \u001b[0;36mcompile_csv_to_netcdf\u001b[0;34m(file_pattern, output_filename, var_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Value'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'LAT'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LON'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_netcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   2108\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mxarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_netcdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2110\u001b[0;31m         return to_netcdf(  # type: ignore[return-value]  # mypy cannot resolve the overloads:(\n\u001b[0m\u001b[1;32m   2111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/writers.py\u001b[0m in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_path\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     store = get_writable_netcdf_store(\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/writers.py\u001b[0m in \u001b[0;36mget_writable_netcdf_store\u001b[0;34m(target, engine, format, mode, autoclose, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_complex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauto_complex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstore_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, auto_complex, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mnetCDF4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_remote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_remote_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/netCDF4_.py\u001b[0m in \u001b[0;36m_acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_acquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m             \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_nc4_require_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquire_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneeds_lock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_File\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;34m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    223\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                     \u001b[0;31m# ensure file doesn't get overridden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32msrc/netCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/content/drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result/CWA_Rainfall_Compiled.nc'"
          ]
        }
      ],
      "source": [
        "# Compiles 24 years of CWA Rainfall and Temperature CSV files\n",
        "# Generates a DEM based on the station grid\n",
        "START_DATE = \"2000-06-01\"\n",
        "END_DATE = \"2023-12-31\"\n",
        "RAIN_PATTERN = \"./drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/TCCIP_Data/Monthly_Average_Rainfall_Data/觀測_月資料_臺灣_降雨量_*.csv\"\n",
        "TEMP_PATTERN = \"./drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/TCCIP_Data/Monthly_Average_Temperature_Data/觀測_月資料_臺灣_平均溫_*.csv\"\n",
        "OUT_RAIN_NC = \"./drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result/CWA_Rainfall_Compiled.nc\"\n",
        "OUT_TEMP_NC = \"./drive/MyDrive/CIE5140_Final_Project/Google_Colab/Database/Running_Result/CWA_Temperature_Compiled.nc\"\n",
        "OUT_DEM_NC = \"Taiwan_DEM.nc\"\n",
        "\n",
        "def compile_csv_to_netcdf(file_pattern, output_filename, var_name):\n",
        "    files = glob.glob(file_pattern)\n",
        "    all_data = []\n",
        "    for f in files:\n",
        "        df = pd.read_csv(f)\n",
        "        df.columns = [c.strip() for c in df.columns] # Standardize column names\n",
        "        # Melt the CSVs\n",
        "        id_vars = ['LON', 'LAT']\n",
        "        val_vars = [c for c in df.columns if c not in id_vars and 'Unnamed' not in c]\n",
        "        df_melt = df.melt(id_vars=id_vars, value_vars=val_vars, var_name='YYYYMM', value_name='Value')\n",
        "        df_melt['Value'] = pd.to_numeric(df_melt['Value'], errors='coerce')\n",
        "\n",
        "        if '降雨量' in file_pattern:\n",
        "            df_melt.loc[df_melt['Value'] < 0, 'Value'] = np.nan\n",
        "        else:\n",
        "            df_melt.loc[df_melt['Value'] <= -50, 'Value'] = np.nan\n",
        "\n",
        "        df_melt['time'] = pd.to_datetime(df_melt['YYYYMM'], format='%Y%m', errors='coerce')\n",
        "        mask = (df_melt['time'] >= START_DATE) & (df_melt['time'] <= END_DATE)\n",
        "        df_filtered = df_melt.loc[mask]\n",
        "        df_filtered = df_melt\n",
        "        if not df_filtered.empty:\n",
        "            all_data.append(df_filtered[['time', 'LAT', 'LON', 'Value']])\n",
        "\n",
        "    full_df = pd.concat(all_data)\n",
        "    full_df = full_df.groupby(['time', 'LAT', 'LON']).mean().reset_index()\n",
        "    ds = full_df.set_index(['time', 'LAT', 'LON']).to_xarray()\n",
        "    ds = ds.rename({'Value': var_name})\n",
        "    ds = ds.rename({'LAT': 'lat', 'LON': 'lon'})\n",
        "    ds.to_netcdf(output_filename)\n",
        "    return ds\n",
        "\n",
        "# #KIWI'S CHANGES\n",
        "# def compile_csv_to_netcdf(file_pattern, output_filename, var_name):\n",
        "#     files = sorted(glob.glob(file_pattern))\n",
        "#     print(f\"[compile_csv_to_netcdf] pattern={file_pattern}\")\n",
        "#     print(f\"[compile_csv_to_netcdf] files found={len(files)} sample={files[:2]}\")\n",
        "\n",
        "#     if len(files) == 0:\n",
        "#         raise FileNotFoundError(f\"glob=0, pattern={file_pattern}\")\n",
        "\n",
        "#     all_data = []\n",
        "#     empty_file_cnt = 0\n",
        "#     time_fail_cnt = 0\n",
        "\n",
        "#     for f in files:\n",
        "#         # 1) read\n",
        "#         df = pd.read_csv(f, encoding=\"utf-8-sig\")\n",
        "#         df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "#         # 2) melt\n",
        "#         id_vars = [\"LON\", \"LAT\"]\n",
        "#         if not all(v in df.columns for v in id_vars):\n",
        "#             raise KeyError(f\"{f} missing LON/LAT. cols={df.columns.tolist()[:20]}\")\n",
        "\n",
        "#         val_vars = [c for c in df.columns if c not in id_vars and \"Unnamed\" not in str(c)]\n",
        "#         if len(val_vars) == 0:\n",
        "#             empty_file_cnt += 1\n",
        "#             print(f\"[skip] {f}: no val_vars\")\n",
        "#             continue\n",
        "\n",
        "#         df_melt = df.melt(id_vars=id_vars, value_vars=val_vars, var_name=\"YYYYMM\", value_name=\"Value\")\n",
        "#         df_melt[\"Value\"] = pd.to_numeric(df_melt[\"Value\"], errors=\"coerce\")\n",
        "\n",
        "#         # ✅ 3) 把 -99.9 當缺值（你檔案就是 -99.9）\n",
        "#         df_melt.loc[df_melt[\"Value\"] <= -99, \"Value\"] = np.nan\n",
        "\n",
        "#         # 4) time parse：欄名是 int 200001 這種 => 轉成字串再 parse\n",
        "#         s = df_melt[\"YYYYMM\"].astype(str).str.strip().str.replace(\"/\", \"\", regex=False).str.replace(\"-\", \"\", regex=False)\n",
        "#         df_melt[\"time\"] = pd.to_datetime(s, format=\"%Y%m\", errors=\"coerce\")\n",
        "\n",
        "#         if df_melt[\"time\"].notna().sum() == 0:\n",
        "#             time_fail_cnt += 1\n",
        "#             print(f\"[skip] {f}: time parse failed, month examples={sorted(set(df_melt['YYYYMM'].astype(str)))[:5]}\")\n",
        "#             continue\n",
        "\n",
        "#         # ✅ 5) 不做 START_DATE/END_DATE 篩選（避免全空）\n",
        "#         df_filtered = df_melt[[\"time\", \"LAT\", \"LON\", \"Value\"]].dropna(subset=[\"time\"])\n",
        "\n",
        "#         # ✅ 6) 只要不是整份都 NaN 就收\n",
        "#         if df_filtered[\"Value\"].notna().sum() == 0:\n",
        "#             # 這種情況通常是整張網格都是 -99.9\n",
        "#             print(f\"[skip] {f}: all values are NaN after replacing -99.9\")\n",
        "#             continue\n",
        "\n",
        "#         all_data.append(df_filtered)\n",
        "\n",
        "#     print(f\"[compile_csv_to_netcdf] appended dfs={len(all_data)} (empty_file={empty_file_cnt}, time_fail={time_fail_cnt})\")\n",
        "\n",
        "#     if len(all_data) == 0:\n",
        "#         raise ValueError(\n",
        "#             \"No objects to concatenate: all files were skipped.\\n\"\n",
        "#             \"Common reasons:\\n\"\n",
        "#             \"1) glob抓到檔但每個檔都沒有月份欄位\\n\"\n",
        "#             \"2) time parse 全失敗\\n\"\n",
        "#             \"3) 全部值都是 -99.9 被轉成 NaN\\n\"\n",
        "#             f\"empty_file_cnt={empty_file_cnt}, time_fail_cnt={time_fail_cnt}\"\n",
        "#         )\n",
        "\n",
        "#     full_df = pd.concat(all_data, ignore_index=True)\n",
        "\n",
        "#     # 7) average duplicates\n",
        "#     full_df = full_df.groupby([\"time\", \"LAT\", \"LON\"], as_index=False)[\"Value\"].mean()\n",
        "\n",
        "#     # 8) to xarray\n",
        "#     ds = full_df.set_index([\"time\", \"LAT\", \"LON\"]).to_xarray()\n",
        "#     ds = ds.rename({\"Value\": var_name, \"LAT\": \"lat\", \"LON\": \"lon\"})\n",
        "\n",
        "#     # 9) write (注意：shared folder 可能 permission denied；output_filename 換到 MyDrive 自己資料夾/或 /content)\n",
        "#     if output_filename:\n",
        "#         ds.to_netcdf(output_filename)\n",
        "\n",
        "#     return ds\n",
        "\n",
        "def generate_dem(reference_ds):\n",
        "    lat = reference_ds.lat\n",
        "    lon = reference_ds.lon\n",
        "    if len(lat.dims) == 1 and len(lon.dims) == 1:   # Handle 1D vs 2D coords\n",
        "        lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')\n",
        "    else:\n",
        "        lat_grid = lat.values\n",
        "        lon_grid = lon.values\n",
        "    # Synthetic elevation, peak at 23.5N, 121.0E\n",
        "    dist = np.sqrt((lat_grid - 23.5)**2 + (lon_grid - 121.0)**2)\n",
        "    elevation = 3500 * np.exp(-dist * 3)\n",
        "    elevation = np.maximum(elevation, 0)\n",
        "\n",
        "    ds_dem = xr.DataArray(\n",
        "        elevation,\n",
        "        coords=reference_ds.isel(time=0).coords,\n",
        "        name='elevation'\n",
        "    ).to_dataset()\n",
        "    ds_dem.to_netcdf(OUT_DEM_NC, mode='w')\n",
        "    return ds_dem\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ds_rain = compile_csv_to_netcdf(RAIN_PATTERN, OUT_RAIN_NC, \"Precip\")\n",
        "    ds_temp = compile_csv_to_netcdf(TEMP_PATTERN, OUT_TEMP_NC, \"Temperature\")\n",
        "    if ds_rain:\n",
        "        generate_dem(ds_rain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTsaHqLsh8o6"
      },
      "source": [
        "### Step 2: Cloud Streaming (ERA5 + IMERG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sKVJe_FdqwX"
      },
      "outputs": [],
      "source": [
        "# ERA5\n",
        "def download_era5(url, key, start_year=2000, end_year=2023):\n",
        "    output_file = \"ERA5_Taiwan_Monthly.nc\"\n",
        "    if os.path.exists(output_file):\n",
        "        return\n",
        "    # Create CDS Client Configuration\n",
        "    with open(os.path.expanduser('~/.cdsapirc'), 'w') as f:\n",
        "        f.write(f\"url: {url}\\nkey: {key}\")\n",
        "\n",
        "    c = cdsapi.Client()\n",
        "    # Split request: Year 2000 (Jun-Dec) + Rest (Jan-Dec)\n",
        "    req_year_2000 = {\n",
        "        'product_type': 'monthly_averaged_reanalysis',\n",
        "        'variable': 'total_precipitation',\n",
        "        'year': '2000',\n",
        "        'month': [str(m).zfill(2) for m in range(6, 13)],\n",
        "        'time': '00:00',\n",
        "        'area': [26, 119, 21, 123],\n",
        "        'format': 'netcdf',\n",
        "    }\n",
        "    req_years_rest = {\n",
        "        'product_type': 'monthly_averaged_reanalysis',\n",
        "        'variable': 'total_precipitation',\n",
        "        'year': [str(y) for y in range(start_year + 1, end_year + 1)],\n",
        "        'month': [str(m).zfill(2) for m in range(1, 13)],\n",
        "        'time': '00:00',\n",
        "        'area': [26, 119, 21, 123],\n",
        "        'format': 'netcdf',\n",
        "    }\n",
        "\n",
        "    if not os.path.exists('ERA5_Part1.nc'):\n",
        "        print(\"  -> Downloading Part 1 (2000 Jun-Dec)...\")\n",
        "        c.retrieve('reanalysis-era5-single-levels-monthly-means', req_year_2000, 'ERA5_Part1.nc')\n",
        "\n",
        "    if not os.path.exists('ERA5_Part2.nc'):\n",
        "        print(f\"  -> Downloading Part 2 ({start_year+1}-{end_year} Full Years)...\")\n",
        "        c.retrieve('reanalysis-era5-single-levels-monthly-means', req_years_rest, 'ERA5_Part2.nc')\n",
        "\n",
        "    ds1 = xr.open_dataset('ERA5_Part1.nc')\n",
        "    ds2 = xr.open_dataset('ERA5_Part2.nc')\n",
        "    if 'valid_time' in ds1.coords:\n",
        "        ds1 = ds1.rename({'valid_time': 'time'})\n",
        "    if 'valid_time' in ds2.coords:\n",
        "        ds2 = ds2.rename({'valid_time': 'time'})\n",
        "    ds = xr.concat([ds1, ds2], dim='time').sortby('time')\n",
        "    ds1.close()\n",
        "    ds2.close()\n",
        "    # Handle 'expver' (occurs if the data includes recent months)\n",
        "    if 'expver' in ds.coords or 'expver' in ds.dims:\n",
        "        try:\n",
        "            # Try combine_first (best for overlapping data), ds.sel(expver=1) is final, ds.sel(expver=5) is preliminary\n",
        "            ds_final = ds.sel(expver=1)\n",
        "            ds_prelim = ds.sel(expver=5)\n",
        "            ds = ds_final.combine_first(ds_prelim)\n",
        "        except:\n",
        "            # Just take the final data if available, or first index\n",
        "            try:\n",
        "                ds = ds.isel(expver=0)\n",
        "            except:\n",
        "                print(\"Continuing with raw data.\")\n",
        "\n",
        "    if not np.issubdtype(ds.time.dtype, np.datetime64):\n",
        "            try: ds['time'] = ds.indexes['time'].to_datetimeindex()\n",
        "            except: pass\n",
        "    raw_times = ds.time.values\n",
        "    new_times = pd.to_datetime(raw_times).to_period('M').to_timestamp()\n",
        "    ds = ds.assign_coords(time=new_times)\n",
        "\n",
        "    unique_times, counts = np.unique(new_times, return_counts=True)\n",
        "    if np.any(counts > 1):\n",
        "        ds = ds.groupby('time').mean(keep_attrs=True)\n",
        "\n",
        "    if 'tp' in ds:\n",
        "        ds['tp'].encoding = {}\n",
        "        target_var = 'tp'\n",
        "    elif 'total_precipitation' in ds:\n",
        "        ds['total_precipitation'].encoding = {}\n",
        "        target_var = 'total_precipitation'\n",
        "\n",
        "    # Total monthly precipitation: (m/day) * 1000 (mm/m) * days_in_a_month = total mm/month\n",
        "    days = ds['time'].dt.days_in_month\n",
        "    if ds[target_var].max() < 100:\n",
        "        ds_mm = ds[target_var] * 1000 * days\n",
        "    else:\n",
        "        ds_mm = ds[target_var]\n",
        "\n",
        "    ds_mm.name = 'total_precipitation'\n",
        "    ds_mm.attrs['units'] = 'mm'\n",
        "    ds_mm.to_netcdf(output_file, encoding={'total_precipitation': {'dtype': 'float32'}})\n",
        "    ds.close()\n",
        "    for f in ['ERA5_Part1.nc', 'ERA5_Part2.nc']:\n",
        "        if os.path.exists(f): os.remove(f)\n",
        "\n",
        "# IMERG\n",
        "def download_imerg(start_year=2000, end_year=2023):\n",
        "    earthaccess.login(strategy=\"interactive\")\n",
        "    output_file = \"IMERG_Taiwan_Monthly_Full.nc\"\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        return\n",
        "\n",
        "    results = earthaccess.search_data(\n",
        "        short_name=\"GPM_3IMERGM\",\n",
        "        version=\"07\",\n",
        "        temporal=(f\"{start_year}-06-01\", f\"{end_year}-12-31\"),\n",
        "        bounding_box=(119, 21, 123, 26)\n",
        "    )\n",
        "    temp_dir = \"imerg_temp_download\"\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    paths = earthaccess.download(results, temp_dir)\n",
        "    datasets = []\n",
        "    for i, file_path in enumerate(paths):\n",
        "        if os.path.getsize(file_path) < 10000:\n",
        "            continue\n",
        "\n",
        "        dt = None\n",
        "        granule = results[i]\n",
        "        try:\n",
        "            date_str = granule[\"umm\"][\"TemporalExtent\"][\"RangeDateTime\"][\"BeginningDateTime\"]\n",
        "            dt = pd.to_datetime(date_str)\n",
        "            if dt.tz is not None: dt = dt.tz_localize(None)\n",
        "        except:\n",
        "            match = re.search(r'(\\d{8})', str(file_path))\n",
        "            if match: dt = pd.to_datetime(match.group(1))\n",
        "\n",
        "        if dt is None:\n",
        "            continue\n",
        "\n",
        "        with h5py.File(file_path, 'r') as f:\n",
        "            grid = f['Grid']\n",
        "            if 'precipitation' in grid:\n",
        "                var_name = 'precipitation'\n",
        "            elif 'precipitationCal' in grid:\n",
        "                var_name = 'precipitationCal'\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            lats = grid['lat'][:]\n",
        "            lons = grid['lon'][:]\n",
        "            lat_idx = np.where((lats >= 21) & (lats <= 26))[0]\n",
        "            lon_idx = np.where((lons >= 119) & (lons <= 123))[0]\n",
        "            if len(lat_idx) == 0 or len(lon_idx) == 0:\n",
        "                continue\n",
        "            lon_start, lon_end = lon_idx.min(), lon_idx.max() + 1\n",
        "            lat_start, lat_end = lat_idx.min(), lat_idx.max() + 1\n",
        "\n",
        "            raw_data = grid[var_name][0, lon_start:lon_end, lat_start:lat_end]\n",
        "            ds_sub = xr.DataArray(\n",
        "                raw_data,\n",
        "                coords={'lon': lons[lon_start:lon_end], 'lat': lats[lat_start:lat_end]},\n",
        "                dims=['lon', 'lat'],\n",
        "                name='precipitation'\n",
        "            )\n",
        "            ds_sub = ds_sub.expand_dims(time=[dt])\n",
        "            datasets.append(ds_sub)\n",
        "\n",
        "            if i % 10 == 0:\n",
        "                print(f\"  Processed {i}/{len(paths)}...\", end='\\r')\n",
        "    ds_combined = xr.concat(datasets, dim='time').sortby('time')\n",
        "\n",
        "    # Conversion: rate (mm/hr) -> total (mm)\n",
        "    days = ds_combined.time.dt.days_in_month\n",
        "    ds_total = ds_combined * 24 * days\n",
        "    ds_total.name = 'precipitation'\n",
        "    ds_total.attrs['units'] = 'mm'\n",
        "    ds_total.to_netcdf(output_file, encoding={'precipitation': {'dtype': 'float32'}})\n",
        "\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    CDS_URL = \"https://cds.climate.copernicus.eu/api\"\n",
        "    CDS_KEY = input(\"Enter your NEW CDS Personal Access Token (or press Enter to skip ERA5): \")\n",
        "    download_era5(CDS_URL, CDS_KEY, 2000, 2023)\n",
        "    download_imerg(2000, 2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh2T_lG8Lgo0"
      },
      "source": [
        "### Step 3: Distributional Diagnosis (Gamma Fitting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiVTRrU6MMWn"
      },
      "outputs": [],
      "source": [
        "# Aligns ERA5 & IMERG to CWA ground truth, performs distribution diagnosis (bias, quantiles, value ranges),\n",
        "# fit multiple distributions (Gamma, LogNorm, Weibull) to find best fit.\n",
        "INPUT_ERA5 = \"ERA5_Taiwan_Monthly.nc\"\n",
        "INPUT_IMERG = \"IMERG_Taiwan_Monthly_Full.nc\"\n",
        "INPUT_CWA = \"CWA_Rainfall_Compiled.nc\"\n",
        "OUTPUT_DIAGNOSIS = \"Taiwan_Rainfall_Diagnosis.nc\"\n",
        "\n",
        "def fix_era5_raw_structure(ds):\n",
        "    if 'expver' in ds.dims:\n",
        "        try:\n",
        "            ds_fixed = ds.sel(expver=1).combine_first(ds.sel(expver=5))\n",
        "            return ds_fixed\n",
        "        except:\n",
        "            return ds.isel(expver=0, drop=True)\n",
        "\n",
        "    return ds\n",
        "\n",
        "def find_best_fit(data, name):\n",
        "    if len(data) == 0:\n",
        "        return None, None, None\n",
        "    valid_data = data[~np.isnan(data)]\n",
        "    valid_data = valid_data[valid_data >= 0]\n",
        "\n",
        "    # These distributions (Gamma/LogNorm/Weibull) are for POSITIVE continuous variables.\n",
        "    # They cannot mathematically handle 0. We fit to the \"Wet\" portion.\n",
        "    wet_data = valid_data[valid_data > 0.1] # Treat <0.1mm as \"Trace/Zero\"\n",
        "    n_zeros = len(valid_data) - len(wet_data)\n",
        "    dry_prob = (n_zeros / len(valid_data)) * 100 if len(valid_data) > 0 else 0\n",
        "    if len(wet_data) < 10:\n",
        "        return None, None, None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Gamma Distribution\n",
        "    params_g = gamma.fit(wet_data, floc=0)\n",
        "    # Calculate Log-Likelihood for AIC comparison (2k - 2logL), lower AIC is better\n",
        "    ll_g = np.sum(np.log(gamma.pdf(wet_data, *params_g)))\n",
        "    k_g = len(params_g) # Number of parameters\n",
        "    aic_g = 2*k_g - 2*ll_g\n",
        "    results['Gamma'] = {'params': params_g, 'aic': aic_g, 'func': gamma}\n",
        "\n",
        "    # Log-Normal Distribution\n",
        "    params_l = lognorm.fit(wet_data, floc=0)\n",
        "    ll_l = np.sum(np.log(lognorm.pdf(wet_data, *params_l)))\n",
        "    k_l = len(params_l)\n",
        "    aic_l = 2*k_l - 2*ll_l\n",
        "    results['LogNorm'] = {'params': params_l, 'aic': aic_l, 'func': lognorm}\n",
        "\n",
        "    # Weibull Distribution\n",
        "    params_w = weibull_min.fit(wet_data, floc=0)\n",
        "    ll_w = np.sum(np.log(weibull_min.pdf(wet_data, *params_w)))\n",
        "    k_w = len(params_w)\n",
        "    aic_w = 2*k_w - 2*ll_w\n",
        "    results['Weibull'] = {'params': params_w, 'aic': aic_w, 'func': weibull_min}\n",
        "\n",
        "    best_dist_name = min(results, key=lambda x: results[x]['aic'])\n",
        "    best_res = results[best_dist_name]\n",
        "    print(f\"{name} AIC Scores: Gamma={aic_g:.0f}, LogNorm={aic_l:.0f}, Weibull={aic_w:.0f}\")\n",
        "    return best_dist_name, best_res['params'], best_res['func'], dry_prob\n",
        "\n",
        "def plot_for_presentation(ds, output_prefix=\"Presentation_Fig\"):\n",
        "    # Spatial Bias Maps: shows bias relative to CWA Ground Truth\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "    bias_imerg = ds['bias_imerg_cwa'].mean(dim='time', skipna=True)\n",
        "    bias_era5 = ds['bias_era5_cwa'].mean(dim='time', skipna=True)\n",
        "    # IMERG Bias\n",
        "    bias_imerg.plot.pcolormesh(ax=axes[0], cmap='RdBu', center=0, cbar_kwargs={'label': 'Bias (mm/month)'})\n",
        "    axes[0].set_title('IMERG Bias (vs CWA Ground Truth)\\n(Blue=Wet, Red=Dry)')\n",
        "    axes[0].set_xlabel('Longitude')\n",
        "    axes[0].set_ylabel('Latitude')\n",
        "    # ERA5 Bias\n",
        "    bias_era5.plot.pcolormesh(ax=axes[1], cmap='RdBu', center=0, cbar_kwargs={'label': 'Bias (mm/month)'})\n",
        "    axes[1].set_title('ERA5 Bias (vs CWA Ground Truth)\\n(Blue=Wet, Red=Dry)')\n",
        "    axes[1].set_xlabel('Longitude')\n",
        "    axes[1].set_ylabel('Latitude')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_1_Bias_Maps.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # Seasonal cycle: average over lat/lon to get a single line for the whole region\n",
        "    season_era = ds['seasonality_era'].mean(dim=['lat', 'lon'], skipna=True)\n",
        "    season_imerg = ds['seasonality_imerg'].mean(dim=['lat', 'lon'], skipna=True)\n",
        "    season_cwa = ds['seasonality_cwa'].mean(dim=['lat', 'lon'], skipna=True)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    season_cwa.plot(label='CWA (Ground Truth)', color='green', linewidth=3, marker='^', linestyle='-')\n",
        "    season_era.plot(label='ERA5 (Reanalysis)', color='blue', linewidth=2, marker='o', linestyle='--')\n",
        "    season_imerg.plot(label='IMERG (Satellite)', color='red', linewidth=2, marker='s', linestyle='--')\n",
        "    plt.title('Average Seasonal Cycle (Taiwan Region)')\n",
        "    plt.ylabel('Precipitation (mm/month)')\n",
        "    plt.xlabel('Month')\n",
        "    plt.xticks(range(1, 13))\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_2_Seasonal_Cycle.png\", dpi=150)\n",
        "    plt.close()\n",
        "    # Distribution fitting (best fit PDF vs empirical histogram)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    era_vals = ds['era5_on_cwa'].values.flatten()\n",
        "    imerg_vals = ds['imerg_on_cwa'].values.flatten()\n",
        "    cwa_vals = ds['cwa_ref'].values.flatten()\n",
        "    cwa_vals = cwa_vals[cwa_vals >= 0]\n",
        "    # Empirical Histograms\n",
        "    plt.hist(cwa_vals, bins=50, density=True, range=(0,800), alpha=0.3, color='green', label='CWA (Ground Truth)')\n",
        "    plt.hist(imerg_vals, bins=50, density=True, range=(0,800), alpha=0.3, color='red', label='IMERG')\n",
        "    plt.hist(era_vals, bins=50, density=True, range=(0,800), alpha=0.3, color='blue', label='ERA5')\n",
        "    x = np.linspace(0, 800, 1000)\n",
        "    # CWA (ground truth)\n",
        "    dist_name_c, params_c, func_c, dry_c = find_best_fit(cwa_vals, \"CWA\")\n",
        "    if func_c:\n",
        "        plt.plot(x, func_c.pdf(x, *params_c), 'g-', lw=2.5, label=f'CWA ({dist_name_c} Fit)')\n",
        "    # ERA5\n",
        "    dist_name_e, params_e, func_e, dry_e = find_best_fit(era_vals, \"ERA5\")\n",
        "    if func_e:\n",
        "        plt.plot(x, func_e.pdf(x, *params_e), 'b--', lw=2, label=f'ERA5 ({dist_name_e} Fit)')\n",
        "    # IMERG\n",
        "    dist_name_i, params_i, func_i, dry_i = find_best_fit(imerg_vals, \"IMERG\")\n",
        "    if func_i:\n",
        "        plt.plot(x, func_i.pdf(x, *params_i), 'r--', lw=2, label=f'IMERG ({dist_name_i} Fit)')\n",
        "    plt.title(f'Rainfall Distribution Fitting (Best Fit Selection)')\n",
        "    plt.xlabel('Monthly Rainfall (mm)')\n",
        "    plt.ylabel('Probability Density')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.xlim(0, 600)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{output_prefix}_3_Distribution_Fit.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"CWA (Truth): {dry_c:.1f}% dry months. Real stations record true '0.0' rain.\")\n",
        "    print(f\"ERA5: {dry_e:.1f}% dry months. Reanalysis tends to generate 'micro-drizzle', rarely 0.\")\n",
        "    print(f\"IMERG: {dry_i:.1f}% dry months. Spatial averaging makes 0.0 rare over large pixels.\")\n",
        "    if dry_e < 0.1:\n",
        "        print(\"ERA5's lack of dry months explains why it prefers Gamma (continuous) while CWA prefers Weibull.\")\n",
        "\n",
        "def run_distribution_diagnosis():\n",
        "    ds_era = xr.open_dataset(INPUT_ERA5)\n",
        "    ds_imerg = xr.open_dataset(INPUT_IMERG)\n",
        "    ds_cwa = None\n",
        "    if os.path.exists(INPUT_CWA):\n",
        "        ds_cwa = xr.open_dataset(INPUT_CWA)\n",
        "\n",
        "    ds_era = fix_era5_raw_structure(ds_era)\n",
        "\n",
        "    if 'latitude' in ds_era.coords:\n",
        "        ds_era = ds_era.rename({'latitude': 'lat'})\n",
        "    if 'longitude' in ds_era.coords:\n",
        "        ds_era = ds_era.rename({'longitude': 'lon'})\n",
        "    if 'LAT' in ds_cwa.coords:\n",
        "        ds_cwa = ds_cwa.rename({'LAT': 'lat'})\n",
        "    if 'LON' in ds_cwa.coords:\n",
        "        ds_cwa = ds_cwa.rename({'LON': 'lon'})\n",
        "\n",
        "    var_e = 'tp' if 'tp' in ds_era else 'total_precipitation'\n",
        "    var_i = 'precipitation' if 'precipitation' in ds_imerg else 'precipitationCal'\n",
        "    var_c = 'Precip'\n",
        "\n",
        "    # Spatial alignment (Regridded to CWA)\n",
        "    ds_era_interp = ds_era[var_e].interp_like(ds_cwa[var_c], method='linear')\n",
        "    ds_imerg_interp = ds_imerg[var_i].interp_like(ds_cwa[var_c], method='linear')\n",
        "    # Calculate bias\n",
        "    bias_imerg = ds_imerg_interp - ds_cwa[var_c]\n",
        "    bias_era5 = ds_era_interp - ds_cwa[var_c]\n",
        "    bias_imerg.name = 'bias_imerg_cwa'\n",
        "    bias_era5.name = 'bias_era5_cwa'\n",
        "\n",
        "    # Seasonality\n",
        "    clim_era = ds_era_interp.groupby('time.month').mean(dim='time')\n",
        "    clim_imerg = ds_imerg_interp.groupby('time.month').mean(dim='time')\n",
        "    clim_cwa = ds_cwa[var_c].groupby('time.month').mean(dim='time')\n",
        "    ds_out = xr.Dataset({\n",
        "        'cwa_ref': ds_cwa[var_c],\n",
        "        'era5_on_cwa': ds_era_interp,\n",
        "        'imerg_on_cwa': ds_imerg_interp,\n",
        "        'bias_imerg_cwa': bias_imerg,\n",
        "        'bias_era5_cwa': bias_era5,\n",
        "        'seasonality_era': clim_era,\n",
        "        'seasonality_imerg': clim_imerg,\n",
        "        'seasonality_cwa': clim_cwa\n",
        "    })\n",
        "    ds_out.to_netcdf(OUTPUT_DIAGNOSIS)\n",
        "    plot_for_presentation(ds_out)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_distribution_diagnosis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLq3eNy4L3bm"
      },
      "source": [
        "### Step 4: Error Modeling (Regression vs. Altitude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds5h0nziepKp"
      },
      "outputs": [],
      "source": [
        "def load_dataset_safe(path):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    with xr.open_dataset(path) as ds:\n",
        "        ds = ds.load()\n",
        "        ds = standardize_coords(ds)\n",
        "        return ds\n",
        "\n",
        "# Align to CWA grid\n",
        "def standardize_coords(ds):\n",
        "    rename = {}\n",
        "    if 'latitude' in ds.coords: rename['latitude'] = 'lat'\n",
        "    if 'longitude' in ds.coords: rename['longitude'] = 'lon'\n",
        "    if rename: ds = ds.rename(rename)\n",
        "    # Force sort to ensure alignment\n",
        "    ds = ds.sortby(['lat', 'lon'])\n",
        "    return ds\n",
        "\n",
        "# Focus: Modeling error variance based on Altitude and Temperature (Static).\n",
        "def run_error_modeling_alt_temp():\n",
        "    ds_rain = load_dataset_safe(OUT_RAIN_NC)\n",
        "    ds_temp = load_dataset_safe(OUT_TEMP_NC)\n",
        "    ds_dem = load_dataset_safe(OUT_DEM_NC)\n",
        "\n",
        "    ds_era = load_dataset_safe(INPUT_ERA5)\n",
        "    ds_imerg = load_dataset_safe(INPUT_IMERG)\n",
        "    ds_era = standardize_coords(ds_era)\n",
        "    ds_imerg = standardize_coords(ds_imerg)\n",
        "\n",
        "    var_rain = 'Precip'\n",
        "    var_temp = 'Temperature'\n",
        "    var_elev = 'elevation'\n",
        "    var_e = 'tp' if 'tp' in ds_era else 'total_precipitation'\n",
        "    var_i = 'precipitation' if 'precipitation' in ds_imerg else 'precipitationCal'\n",
        "\n",
        "    # Interpolation to rain grid\n",
        "    era_aligned = ds_era[var_e].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    imerg_aligned = ds_imerg[var_i].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    dem_aligned = ds_dem[var_elev].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='nearest')\n",
        "    # Calculate mean temp to rain grid\n",
        "    temp_mean = ds_temp[var_temp].mean(dim='time', skipna=True)\n",
        "    temp_aligned = temp_mean.interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    dem_aligned = dem_aligned.transpose('lat', 'lon')\n",
        "    temp_aligned = temp_aligned.transpose('lat', 'lon')\n",
        "    era_aligned = era_aligned.transpose('time', 'lat', 'lon')\n",
        "    imerg_aligned = imerg_aligned.transpose('time', 'lat', 'lon')\n",
        "\n",
        "    # Calculate static error variance\n",
        "    common_time = np.intersect1d(ds_rain.time, era_aligned.time)\n",
        "    resid_era = era_aligned.sel(time=common_time) - ds_rain[var_rain].sel(time=common_time)\n",
        "    resid_imerg = imerg_aligned.sel(time=common_time) - ds_rain[var_rain].sel(time=common_time)\n",
        "    # Variance over all time\n",
        "    sigma2_era = resid_era.var(dim='time', skipna=True)\n",
        "    sigma2_imerg = resid_imerg.var(dim='time', skipna=True)\n",
        "    sigma2_era = sigma2_era.transpose('lat', 'lon')\n",
        "    sigma2_imerg = sigma2_imerg.transpose('lat', 'lon')\n",
        "    # Debug Shapes\n",
        "    print(f\"   DEBUG SHAPES (Post-Transpose):\")\n",
        "    print(f\"   Sigma ERA: {sigma2_era.shape}\")\n",
        "    print(f\"   DEM:       {dem_aligned.shape}\")\n",
        "    print(f\"   Temp:      {temp_aligned.shape}\")\n",
        "\n",
        "    # Regression\n",
        "    def fit_and_predict(sigma_map):\n",
        "        # Stacking (lat, lon) into a single dimension 'z' ensures pixels stay matched\n",
        "        y_stacked = sigma_map.stack(z=('lat', 'lon'))\n",
        "        x1_stacked = dem_aligned.stack(z=('lat', 'lon'))\n",
        "        x2_stacked = temp_aligned.stack(z=('lat', 'lon'))\n",
        "        y = y_stacked.values\n",
        "        x1 = x1_stacked.values\n",
        "        x2 = x2_stacked.values\n",
        "        # Unified Masking\n",
        "        mask = ~np.isnan(y) & ~np.isnan(x1) & ~np.isnan(x2) & ~np.isinf(y)\n",
        "\n",
        "        # Train\n",
        "        y_train = y[mask]\n",
        "        X_train = np.column_stack((x1[mask], x2[mask]))\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "        a, b = model.coef_\n",
        "        c = model.intercept_\n",
        "        r2 = r2_score(y_train, model.predict(X_train))\n",
        "        print(f\"σ² = ({a:.2f} * Alt) + ({b:.2f} * Temp) + {c:.2f}\")\n",
        "        print(f\"R^2: {r2:.4f}\")\n",
        "\n",
        "        # Predict full map\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_full = np.column_stack((x1, x2))\n",
        "        X_full_imputed = imputer.fit_transform(X_full)\n",
        "        y_pred_flat = model.predict(X_full_imputed)\n",
        "        # Reconstruct 2D Map by unstacking\n",
        "        da_pred = xr.DataArray(y_pred_flat, coords=y_stacked.coords, dims=y_stacked.dims)\n",
        "        y_pred_map = da_pred.unstack('z')\n",
        "        # Mask Ocean based on original DEM\n",
        "        y_pred_map = y_pred_map.where(~np.isnan(dem_aligned))\n",
        "        return model, y_pred_map\n",
        "\n",
        "    _, pred_map_e = fit_and_predict(sigma2_era)\n",
        "    _, pred_map_i = fit_and_predict(sigma2_imerg)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    x_flat = dem_aligned.stack(z=('lat', 'lon')).values\n",
        "    t_flat = temp_aligned.stack(z=('lat', 'lon')).values\n",
        "    y_era_flat = sigma2_era.stack(z=('lat', 'lon')).values\n",
        "    y_img_flat = sigma2_imerg.stack(z=('lat', 'lon')).values\n",
        "    # Plot 1: Altitude vs Error\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.scatter(x_flat, y_era_flat, alpha=0.3, s=5, c='blue', label='ERA5')\n",
        "    plt.scatter(x_flat, y_img_flat, alpha=0.3, s=5, c='red', label='IMERG')\n",
        "    plt.xlabel('Altitude (m)')\n",
        "    plt.ylabel('Error Variance')\n",
        "    plt.title('Error vs Altitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    # Plot 2: Temp vs Error\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.scatter(t_flat, y_era_flat, alpha=0.3, s=5, c='blue', label='ERA5')\n",
        "    plt.scatter(t_flat, y_img_flat, alpha=0.3, s=5, c='red', label='IMERG')\n",
        "    plt.xlabel('Mean Temp (°C)')\n",
        "    plt.ylabel('Error Variance')\n",
        "    plt.title('Error vs Temperature')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    # Plot 3: Predicted Map IMERG (RdBu_r)\n",
        "    if pred_map_i is not None:\n",
        "        plt.subplot(2, 2, 3)\n",
        "        # Transpose back to (lat, lon) for plotting just in case unstack flipped it\n",
        "        if 'lat' in pred_map_i.dims and 'lon' in pred_map_i.dims:\n",
        "             pred_map_i = pred_map_i.transpose('lat', 'lon')\n",
        "        plt.imshow(np.flipud(pred_map_i.values), cmap='magma')\n",
        "        plt.title('Predicted Error Map (IMERG)')\n",
        "        plt.colorbar(label='Predicted σ²')\n",
        "        plt.axis('off')\n",
        "    # Plot 4: Predicted Map ERA5 (RdBu_r)\n",
        "    if pred_map_e is not None:\n",
        "        plt.subplot(2, 2, 4)\n",
        "        if 'lat' in pred_map_e.dims and 'lon' in pred_map_e.dims:\n",
        "             pred_map_e = pred_map_e.transpose('lat', 'lon')\n",
        "        plt.imshow(np.flipud(pred_map_e.values), cmap='magma')\n",
        "        plt.title('Predicted Error Map (ERA5)')\n",
        "        plt.colorbar(label='Predicted σ²')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Presentation_Fig_4_Static_Error.png\", dpi=150)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_error_modeling_alt_temp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5vMO1lKMRrs"
      },
      "outputs": [],
      "source": [
        "# Focus: Modeling error variance based on Altitude, Temperature, and Seasonality.\n",
        "def run_error_modeling_alt_temp_seas():\n",
        "    ds_rain = load_dataset_safe(OUT_RAIN_NC)\n",
        "    ds_temp = load_dataset_safe(OUT_TEMP_NC)\n",
        "    ds_dem = load_dataset_safe(OUT_DEM_NC)\n",
        "\n",
        "    ds_era = load_dataset_safe(INPUT_ERA5)\n",
        "    ds_imerg = load_dataset_safe(INPUT_IMERG)\n",
        "    ds_era = standardize_coords(ds_era)\n",
        "    ds_imerg = standardize_coords(ds_imerg)\n",
        "\n",
        "    # Variables\n",
        "    var_rain = 'Precip'\n",
        "    var_temp = 'Temperature'\n",
        "    var_elev = 'elevation'\n",
        "    # Robust variable finding for ERA5/IMERG\n",
        "    var_e = 'tp' if 'tp' in ds_era else 'total_precipitation'\n",
        "    var_i = 'precipitation' if 'precipitation' in ds_imerg else 'precipitationCal'\n",
        "\n",
        "    # Interpolate models to CWA Grid\n",
        "    era_aligned = ds_era[var_e].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    imerg_aligned = ds_imerg[var_i].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    # Align DEM and Temp to CWA grid\n",
        "    dem_aligned = ds_dem[var_elev].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='nearest')\n",
        "    temp_mean = ds_temp[var_temp].mean(dim='time', skipna=True)\n",
        "    temp_aligned = temp_mean.interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "\n",
        "    # Ensure time dimensions align if they differ slightly\n",
        "    common_time = np.intersect1d(ds_rain.time, era_aligned.time)\n",
        "    resid_era = era_aligned.sel(time=common_time) - ds_rain[var_rain].sel(time=common_time)\n",
        "    resid_imerg = imerg_aligned.sel(time=common_time) - ds_rain[var_rain].sel(time=common_time)\n",
        "\n",
        "    # Calculate variance per month, instead of one sigma^2 per pixel, we get 12 sigma^2 maps (one for Jan, Feb...)\n",
        "    sigma2_era = resid_era.groupby('time.month').var(dim='time', skipna=True)\n",
        "    sigma2_imerg = resid_imerg.groupby('time.month').var(dim='time', skipna=True)\n",
        "\n",
        "    # Regression with seasonality\n",
        "    def solve_regression_seasonal(sigma_map):\n",
        "        # Prepare Data\n",
        "        # We need to flatten (Month, Lat, Lon) -> 1D Arrays\n",
        "        # X1: Elevation (Repeated for 12 months)\n",
        "        # X2: Temperature (Repeated for 12 months) - ideally we'd use monthly temp, but mean is ok for spatial trend\n",
        "        # X3: Month Sin\n",
        "        # X4: Month Cos\n",
        "        months = sigma_map.month.values\n",
        "        n_months = len(months)\n",
        "\n",
        "        # Expand static maps to 3D (12, Lat, Lon)\n",
        "        dem_3d = np.tile(dem_aligned.values, (n_months, 1, 1))\n",
        "        temp_3d = np.tile(temp_aligned.values, (n_months, 1, 1))\n",
        "        # Create Seasonality Maps\n",
        "        month_grid = np.tile(months[:, None, None], (1, dem_aligned.shape[0], dem_aligned.shape[1]))\n",
        "        sin_month = np.sin(2 * np.pi * month_grid / 12)\n",
        "        cos_month = np.cos(2 * np.pi * month_grid / 12)\n",
        "\n",
        "        y = sigma_map.values.flatten()\n",
        "        x1 = dem_3d.flatten()\n",
        "        x2 = temp_3d.flatten()\n",
        "        x3 = sin_month.flatten()\n",
        "        x4 = cos_month.flatten()\n",
        "\n",
        "        # Mask NaNs\n",
        "        mask = ~np.isnan(y) & ~np.isnan(x1) & ~np.isnan(x2) & ~np.isinf(y)\n",
        "        y_clean = y[mask]\n",
        "        X_clean = np.column_stack((x1[mask], x2[mask], x3[mask], x4[mask]))\n",
        "\n",
        "        # Regression\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_clean, y_clean)\n",
        "        a, b, c, d = model.coef_\n",
        "        intercept = model.intercept_\n",
        "        r2 = r2_score(y_clean, model.predict(X_clean))\n",
        "        print(f\"Equation: σ² = ({a:.2f}*Alt) + ({b:.2f}*Temp) + ({c:.2f}*sinM) + ({d:.2f}*cosM) + {intercept:.2f}\")\n",
        "        print(f\"R^2: {r2:.4f}\")\n",
        "\n",
        "        # Predict Map (Just for Month 8 - August/Typhoon Season as example)\n",
        "        # We construct a 2D map for a specific month to visualize\n",
        "        target_month = 8\n",
        "\n",
        "        dem_2d = dem_aligned.values.flatten()\n",
        "        temp_2d = temp_aligned.values.flatten()\n",
        "        sin_2d = np.full_like(dem_2d, np.sin(2 * np.pi * target_month / 12))\n",
        "        cos_2d = np.full_like(dem_2d, np.cos(2 * np.pi * target_month / 12))\n",
        "\n",
        "        imputer = SimpleImputer(strategy='mean')\n",
        "        X_aug = np.column_stack((dem_2d, temp_2d, sin_2d, cos_2d))\n",
        "        X_aug_imp = imputer.fit_transform(X_aug)\n",
        "\n",
        "        y_pred_aug = model.predict(X_aug_imp)\n",
        "        y_pred_aug[np.isnan(dem_2d)] = np.nan # Mask ocean\n",
        "\n",
        "        return y_pred_aug.reshape(dem_aligned.shape)\n",
        "\n",
        "    pred_map_e = solve_regression_seasonal(sigma2_era)\n",
        "    pred_map_i = solve_regression_seasonal(sigma2_imerg)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Plot 1: Predicted Reliability Map (IMERG - August)\n",
        "    if pred_map_i is not None:\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(np.flipud(pred_map_i), cmap='magma')\n",
        "        plt.title('Predicted Error Map (IMERG) - August\\n(Including Seasonality)')\n",
        "        plt.colorbar(label='Predicted σ²')\n",
        "        plt.axis('off')\n",
        "    # Plot 2: Predicted Reliability Map (ERA5 - August)\n",
        "    if pred_map_e is not None:\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(np.flipud(pred_map_e), cmap='magma')\n",
        "        plt.title('Predicted Error Map (ERA5) - August\\n(Including Seasonality)')\n",
        "        plt.colorbar(label='Predicted σ²')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Presentation_Fig_4_Error_Analysis_Seasonal.png\", dpi=150)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_error_modeling_alt_temp_seas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9jabO_OMBt1"
      },
      "source": [
        "### Step 5: Bayesian Fusion (Merging Datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6OzDQV6MTuI"
      },
      "outputs": [],
      "source": [
        "# Global Inverse Variance Weighting:\n",
        "# Since ERA5 has lower global RMSE than IMERG (Phase 6), it gets slightly more weight.\n",
        "# This is safer than pixel-wise weighting which caused overfitting.\n",
        "INPUT_CWA_TEMP = \"CWA_Temperature_Compiled.nc\"\n",
        "INPUT_DEM = \"Taiwan_DEM.nc\"\n",
        "OUTPUT_FUSED = \"Taiwan_Rainfall_Fused.nc\"\n",
        "OUTPUT_CSV = \"Fusion_Variogram_Data.csv\" # New Output\n",
        "\n",
        "def calculate_variogram(data_map, n_bins=20):\n",
        "    vals = data_map.values.flatten()\n",
        "    lats = data_map.lat.values\n",
        "    lons = data_map.lon.values\n",
        "    # Handle coordinates if 1D or 2D\n",
        "    if lats.ndim == 1 and lons.ndim == 1:\n",
        "        lat_grid, lon_grid = np.meshgrid(lats, lons, indexing='ij')\n",
        "    else:\n",
        "        lat_grid, lon_grid = lats, lons\n",
        "    mask = ~np.isnan(vals)\n",
        "    z = vals[mask]\n",
        "    coords = np.column_stack((lat_grid.flatten()[mask], lon_grid.flatten()[mask]))\n",
        "\n",
        "    # Pairwise distances (Euclidean approx)\n",
        "    # Using a subset if too large to prevent memory crash\n",
        "    if len(z) > 5000:\n",
        "        idx = np.random.choice(len(z), 5000, replace=False)\n",
        "        z = z[idx]\n",
        "        coords = coords[idx]\n",
        "\n",
        "    dists = pdist(coords)\n",
        "    sq_diffs = pdist(z[:, None], metric='sqeuclidean')\n",
        "\n",
        "    # Binning\n",
        "    bins = np.linspace(0, np.max(dists)/3, n_bins)\n",
        "    bin_centers = 0.5 * (bins[1:] + bins[:-1])\n",
        "    gamma = []\n",
        "    inds = np.digitize(dists, bins)\n",
        "    for i in range(1, len(bins)):\n",
        "        var_val = 0.5 * np.mean(sq_diffs[inds == i]) if np.any(inds == i) else np.nan\n",
        "        gamma.append(var_val)\n",
        "\n",
        "    return bin_centers, np.array(gamma)\n",
        "\n",
        "# Spatial Fusion\n",
        "def run_data_fusion():\n",
        "    ds_rain = load_dataset_safe(INPUT_CWA)\n",
        "    ds_era = load_dataset_safe(INPUT_ERA5)\n",
        "    ds_imerg = load_dataset_safe(INPUT_IMERG)\n",
        "\n",
        "    var_c = 'Precip'\n",
        "    var_e = 'tp' if 'tp' in ds_era else 'total_precipitation'\n",
        "    var_i = 'precipitation' if 'precipitation' in ds_imerg else 'precipitationCal'\n",
        "\n",
        "    era_aligned = ds_era[var_e].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "    imerg_aligned = ds_imerg[var_i].interp(lat=ds_rain.lat, lon=ds_rain.lon, method='linear')\n",
        "\n",
        "    common_time = np.intersect1d(ds_rain.time, era_aligned.time)\n",
        "    print(f\"   -> Found {len(common_time)} common months.\")\n",
        "\n",
        "    # Bias Correction\n",
        "    # Calculate mean bias over time (Climatological Bias)\n",
        "    bias_era = (era_aligned.sel(time=common_time) - ds_rain[var_c].sel(time=common_time)).mean(dim='time')\n",
        "    bias_imerg = (imerg_aligned.sel(time=common_time) - ds_rain[var_c].sel(time=common_time)).mean(dim='time')\n",
        "    # Correct the models (Unbiased Models)\n",
        "    era_corrected = era_aligned - bias_era\n",
        "    imerg_corrected = imerg_aligned - bias_imerg\n",
        "    era_corrected = era_corrected.where(era_corrected >= 0, 0)\n",
        "    imerg_corrected = imerg_corrected.where(imerg_corrected >= 0, 0)\n",
        "\n",
        "    # Optimized Global Weight Fusion\n",
        "    # We use the RMSE results from Phase 6 to determine global weights.\n",
        "    # W = 1 / MSE\n",
        "    resid_era = (era_corrected.sel(time=common_time) - ds_rain[var_c].sel(time=common_time))\n",
        "    resid_imerg = (imerg_corrected.sel(time=common_time) - ds_rain[var_c].sel(time=common_time))\n",
        "    # Global MSE\n",
        "    mse_era = (resid_era**2).mean().item()\n",
        "    mse_imerg = (resid_imerg**2).mean().item()\n",
        "    print(f\"ERA5 MSE: {mse_era:.2f}\")\n",
        "    print(f\"IMERG MSE: {mse_imerg:.2f}\")\n",
        "    # Inverse Variance Weights\n",
        "    w_era_val = 1.0 / mse_era\n",
        "    w_imerg_val = 1.0 / mse_imerg\n",
        "    # Normalize\n",
        "    w_sum = w_era_val + w_imerg_val\n",
        "    W_ERA = w_era_val / w_sum\n",
        "    W_IMERG = w_imerg_val / w_sum\n",
        "    print(f\"Optimal weights: ERA5={W_ERA:.3f}, IMERG={W_IMERG:.3f}\")\n",
        "\n",
        "    ds_fused_raw = (W_ERA * era_corrected) + (W_IMERG * imerg_corrected)\n",
        "    fused_bias = (ds_fused_raw.sel(time=common_time) - ds_rain[var_c].sel(time=common_time)).mean(dim='time')\n",
        "    ds_fused = ds_fused_raw - fused_bias\n",
        "    ds_fused = ds_fused.where(ds_fused >= 0, 0)\n",
        "    ds_fused.name = 'precip_fused'\n",
        "    ds_fused.to_netcdf(OUTPUT_FUSED)\n",
        "\n",
        "    # Spatial check (variogram)\n",
        "    mean_obs = ds_rain[var_c].mean(dim='time', skipna=True)\n",
        "\n",
        "    bins_cwa, gam_cwa = calculate_variogram(mean_obs)\n",
        "    bins_era, gam_era = calculate_variogram(era_corrected.mean(dim='time'))\n",
        "    bins_img, gam_imerg = calculate_variogram(imerg_corrected.mean(dim='time'))\n",
        "    bins_fus, gam_fused = calculate_variogram(ds_fused.mean(dim='time'))\n",
        "\n",
        "    if bins_cwa is not None:\n",
        "        df_var = pd.DataFrame({\n",
        "            'Distance_Deg': bins_cwa,\n",
        "            'Semivariance_CWA': gam_cwa,\n",
        "            'Semivariance_ERA5': gam_era if bins_era is not None else np.nan,\n",
        "            'Semivariance_IMERG': gam_imerg if bins_img is not None else np.nan,\n",
        "            'Semivariance_FUSED': gam_fused if bins_fus is not None else np.nan\n",
        "        })\n",
        "        df_var.to_csv(OUTPUT_CSV, index=False)\n",
        "\n",
        "    # Plot 1: Sptial Maps\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    # Prepare Mean Maps for Visualization\n",
        "    map_cwa = mean_obs\n",
        "    map_era = era_corrected.mean(dim='time', skipna=True)\n",
        "    map_imerg = imerg_corrected.mean(dim='time', skipna=True)\n",
        "    map_fused = ds_fused.mean(dim='time', skipna=True)\n",
        "\n",
        "    vmin = min(map_cwa.min(), map_era.min(), map_imerg.min(), map_fused.min())\n",
        "    vmax = max(map_cwa.max(), map_era.max(), map_imerg.max(), map_fused.max())\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    map_cwa.plot(cmap='Blues', vmin=vmin, vmax=vmax, cbar_kwargs={'label': 'mm/month'})\n",
        "    plt.title('GROUND TRUTH (CWA)\\n(Target Pattern)')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    map_era.plot(cmap='Blues', vmin=vmin, vmax=vmax, cbar_kwargs={'label': 'mm/month'})\n",
        "    plt.title(f'ERA5 Corrected (W={W_ERA:.2f})')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    map_imerg.plot(cmap='Blues', vmin=vmin, vmax=vmax, cbar_kwargs={'label': 'mm/month'})\n",
        "    plt.title(f'IMERG Corrected (W={W_IMERG:.2f})')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    map_fused.plot(cmap='Blues', vmin=vmin, vmax=vmax, cbar_kwargs={'label': 'mm/month'})\n",
        "    plt.title('FINAL FUSED PRODUCT\\n(Optimal Global Weight)')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Presentation_Fig_5a_Spatial_Maps.png\", dpi=150)\n",
        "\n",
        "    # Plot 2: Variogram comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    if bins_era is not None: plt.plot(bins_era, gam_era, 'b--.', label=f'ERA5 (W={W_ERA:.2f})')\n",
        "    if bins_img is not None: plt.plot(bins_img, gam_imerg, 'r--.', label=f'IMERG (W={W_IMERG:.2f})')\n",
        "    if bins_fus is not None: plt.plot(bins_fus, gam_fused, 'g-^', linewidth=2, label='Fused Product Texture')\n",
        "\n",
        "    plt.xlabel('Distance (Degrees)')\n",
        "    plt.ylabel('Semivariance (Spatial Variability)')\n",
        "    plt.title('Spatial Structure Comparison (Variogram)\\n(Goal: Green line should match Black line)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Presentation_Fig_5b_Variogram.png\", dpi=150)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_data_fusion()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXd5sjq3MHHe"
      },
      "source": [
        "### Step 6: Validation (Bootstrapping & Final Stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKPKIQGmKMmF"
      },
      "outputs": [],
      "source": [
        "# Goal: Prove that Fused Data is statistically significantly better than ERA5/IMERG.\n",
        "# Technique: Bootstrapping (10000x) & Hypothesis Testing (t-test).\n",
        "INPUT_FUSED = \"Taiwan_Rainfall_Fused.nc\"\n",
        "BOOTSTRAP_N = 10000 # Increased for higher precision\n",
        "TEST_SPLIT_RATIO = 0.20 # 80/20 train/val split\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def load_and_align():\n",
        "    \"\"\"\n",
        "    Loads all datasets and aligns them to the CWA grid for pixel-to-pixel comparison.\n",
        "    \"\"\"\n",
        "    # Load CWA (Ground Truth)\n",
        "    ds_cwa = load_dataset_safe(INPUT_CWA)\n",
        "    # Load Models\n",
        "    ds_era = load_dataset_safe(INPUT_ERA5)\n",
        "    ds_imerg = load_dataset_safe(INPUT_IMERG)\n",
        "    ds_fused = load_dataset_safe(INPUT_FUSED)\n",
        "\n",
        "    # Variables\n",
        "    v_c = 'Precip'\n",
        "    v_e = 'tp' if 'tp' in ds_era else 'total_precipitation'\n",
        "    v_i = 'precipitation' if 'precipitation' in ds_imerg else 'precipitationCal'\n",
        "    v_f = 'precip_fused'\n",
        "\n",
        "    era_a = ds_era[v_e].interp(lat=ds_cwa.lat, lon=ds_cwa.lon, method='linear')\n",
        "    imerg_a = ds_imerg[v_i].interp(lat=ds_cwa.lat, lon=ds_cwa.lon, method='linear')\n",
        "    fused_a = ds_fused[v_f].interp(lat=ds_cwa.lat, lon=ds_cwa.lon, method='nearest') # Fused is already on grid\n",
        "\n",
        "    common_time = np.intersect1d(ds_cwa.time, era_a.time)\n",
        "    # Flatten Data for Statistical Testing (Vector of all pixels, all times)\n",
        "    # This creates a massive 1D array of every observation\n",
        "    def flatten_data(da):\n",
        "        return da.sel(time=common_time).values.flatten()\n",
        "    cwa_flat = flatten_data(ds_cwa[v_c])\n",
        "    era_flat = flatten_data(era_a)\n",
        "    imerg_flat = flatten_data(imerg_a)\n",
        "    fused_flat = flatten_data(fused_a)\n",
        "    mask = ~np.isnan(cwa_flat) & ~np.isnan(era_flat) & ~np.isnan(imerg_flat) & ~np.isnan(fused_flat)\n",
        "    return cwa_flat[mask], era_flat[mask], imerg_flat[mask], fused_flat[mask]\n",
        "\n",
        "def bootstrap_rmse(truth, model, n_boot=1000):\n",
        "    errors = (model - truth) ** 2\n",
        "    rmse_list = []\n",
        "\n",
        "    # Resample indices with replacement\n",
        "    indices = np.arange(len(truth))\n",
        "\n",
        "    for _ in range(n_boot):\n",
        "        # Sample indices\n",
        "        boot_idx = resample(indices, replace=True, n_samples=len(truth))\n",
        "        mse = np.mean(errors[boot_idx])\n",
        "        rmse_list.append(np.sqrt(mse))\n",
        "\n",
        "    return np.array(rmse_list)\n",
        "\n",
        "def run_validation():\n",
        "    obs, mod_era, mod_imerg, mod_fused = load_and_align()\n",
        "\n",
        "    # 2. TRAIN/TEST SPLIT\n",
        "    # We simulate a \"held-out\" validation set.\n",
        "    # Since we already fused using all data in Phase 5 (for the map), strictly speaking\n",
        "    # this is an \"in-sample\" check unless we re-ran Ph5.\n",
        "    # However, for demonstrating the STATISTICAL METHOD, we will split the *residuals*.\n",
        "    n_samples = len(obs)\n",
        "    n_val = int(n_samples * TEST_SPLIT_RATIO)\n",
        "    # Randomly select 20% indices for validation\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    val_indices = np.random.choice(n_samples, n_val, replace=False)\n",
        "    val_obs = obs[val_indices]\n",
        "    val_era = mod_era[val_indices]\n",
        "    val_imerg = mod_imerg[val_indices]\n",
        "    val_fused = mod_fused[val_indices]\n",
        "\n",
        "    # Bootstrapping RMSE\n",
        "    rmse_boot_era = bootstrap_rmse(val_obs, val_era, BOOTSTRAP_N)\n",
        "    rmse_boot_imerg = bootstrap_rmse(val_obs, val_imerg, BOOTSTRAP_N)\n",
        "    rmse_boot_fused = bootstrap_rmse(val_obs, val_fused, BOOTSTRAP_N)\n",
        "\n",
        "    # Confidence Intervals\n",
        "    def get_ci(boot_dist):\n",
        "        return np.percentile(boot_dist, [2.5, 97.5]), np.mean(boot_dist)\n",
        "    ci_era, m_era = get_ci(rmse_boot_era)\n",
        "    ci_img, m_img = get_ci(rmse_boot_imerg)\n",
        "    ci_fus, m_fus = get_ci(rmse_boot_fused)\n",
        "\n",
        "    print(f\"ERA5 RMSE:  {m_era:.2f} [{ci_era[0]:.2f}, {ci_era[1]:.2f}] mm\")\n",
        "    print(f\"IMERG RMSE: {m_img:.2f} [{ci_img[0]:.2f}, {ci_img[1]:.2f}] mm\")\n",
        "    print(f\"FUSED RMSE: {m_fus:.2f} [{ci_fus[0]:.2f}, {ci_fus[1]:.2f}] mm\")\n",
        "\n",
        "    # Hypothesis Testing\n",
        "    # H0: Fused Error >= ERA5 Error\n",
        "    # H1: Fused Error < ERA5 Error (One-sided test)\n",
        "    # We treat the bootstrap distributions as the samples for the t-test\n",
        "    # Test vs ERA5\n",
        "    t_stat, p_val = stats.ttest_ind(rmse_boot_fused, rmse_boot_era, alternative='less')\n",
        "    is_sig = p_val < 0.05\n",
        "    print(f\"   vs ERA5:  t={t_stat:.2f}, p={p_val:.4e} | Significant? {'Yes' if is_sig else 'No'}\")\n",
        "    # Test vs IMERG\n",
        "    t_stat2, p_val2 = stats.ttest_ind(rmse_boot_fused, rmse_boot_imerg, alternative='less')\n",
        "    is_sig2 = p_val2 < 0.05\n",
        "    print(f\"   vs IMERG: t={t_stat2:.2f}, p={p_val2:.4e} | Significant? {'Yes' if is_sig2 else 'No'}\")\n",
        "\n",
        "    # Plot Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(rmse_boot_era, bins=30, alpha=0.5, label='ERA5 Error', color='blue')\n",
        "    plt.hist(rmse_boot_imerg, bins=30, alpha=0.5, label='IMERG Error', color='red')\n",
        "    plt.hist(rmse_boot_fused, bins=30, alpha=0.7, label='FUSED Error', color='green')\n",
        "    plt.axvline(m_fus, color='green', linestyle='--', linewidth=2, label='Fused Mean')\n",
        "    plt.axvline(m_era, color='blue', linestyle='--', linewidth=2)\n",
        "    plt.xlabel('RMSE (mm/month)')\n",
        "    plt.ylabel('Frequency (Bootstrap Samples)')\n",
        "    plt.title(f'Improvement Verification: Fused Model vs Inputs\\n(p-value vs ERA5: {p_val:.1e})')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"Presentation_Fig_6_Validation.png\", dpi=150)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_validation()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}